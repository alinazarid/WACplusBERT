{"cells":[{"cell_type":"markdown","metadata":{"id":"1PSppxe8mfdN"},"source":["# Fine Tuneing BERT with WAC and Concreteness Score\n","There reference for this notebook is found at [here](https://medium.com/analytics-vidhya/finetune-distilbert-for-multi-label-text-classsification-task-994eb448f94c) and at [this repository](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/Transformers_multilabel_distilbert.ipynb). To incorporate the GLUE tasks into the script, this [link](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb?authuser=1#scrollTo=imY1oC3SIrJf) is used.\n","\n","\n","**In this notebook we focsus on binary classification tasks although the notebook is setup such that it is easy to adopt non-binary tasks by only picking the right loss function.**"]},{"cell_type":"markdown","metadata":{"id":"GwPLadphmfdS"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAFHZ-vwnmlJ"},"outputs":[],"source":["# # For running on Colab\n","# from google.colab import drive\n","# import os\n","# drive.mount('/content/drive')\n","# os.chdir(r\"drive/MyDrive/Colab Notebooks/wac_bert\")  # May need to change based on the user"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QglYMbJ46-nu"},"outputs":[],"source":["# # Installing Requirements\n","# !pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n","# !pip install -r requirements.txt\n","# !pip install ./wac_bert_lib/wac_bert_tools-0.1.0-py3-none-any.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyRJW6immfdT"},"outputs":[],"source":["import os\n","import pickle as pkl\n","import numpy as np\n","import pandas as pd\n","import random\n","import warnings\n","from collections import defaultdict\n","from sklearn.preprocessing import OneHotEncoder\n","from datasets import load_dataset, load_metric\n","\n","import transformers\n","import torch\n","from tqdm import tqdm_notebook as tqdm\n","\n","from wac_bert_tools import wac, concreteness\n","from wac_bert_tools import registrar, tokenize\n","\n","warnings.filterwarnings('ignore')\n","transformers.logging.set_verbosity_error()\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"S05cQQM5mfdU"},"source":["## Parameters\n","pick bert like model at https://huggingface.co/models\n","\n","`GLUE_TASKS = [\"cola\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"wnli\", \"mnli\", \"mnli-mm\", \"stsb\"]`\n","\n","`BINARY_GLUE_TASKS = [\"cola\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"wnli\"]`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLF3EWDamfdU"},"outputs":[],"source":["task='rte'\n","model_checkpoint = 'distilbert-base-uncased'\n","WAC_METHOD = ''#'' or 'CAT' or 'ADD'\n","MATCH_WAC = False  # Resizes BERT output using a MLP to match smaller WAC vectors.\n","FIXED_SEED = True\n","TEST=False # True uses 10% of the data\n","# Model Perameters\n","MAX_LEN = 128\n","BATCH_SIZE = 32\n","EPOCHS = 10\n","LEARNING_RATE = 2e-05\n","DROPOUT=0.5\n","\n","tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n","if FIXED_SEED:\n","    seed_val = 2021\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)\n","reg_arg={\n","        'model':model_checkpoint,\n","        'tokenizer':tokenizer.name_or_path,\n","        'data': task+('_sharded' if TEST else ''),\n","        'epoch': EPOCHS,\n","        'max_len': MAX_LEN, \n","        'batch_Size': BATCH_SIZE,\n","        'learning_rate': LEARNING_RATE,\n","        'fixed_seed':FIXED_SEED,\n","        'loss_name':'',\n","        'max_epoch':EPOCHS\n","    }\n","if WAC_METHOD=='CAT': MATCH_WAC = True"]},{"cell_type":"markdown","metadata":{"id":"gDMoUg8pmfdX"},"source":["## Data\n","\n","* Input Data\n","    - Concreteness Data\n","    - WAC vectors Data\n","* Training Data\n","    - DataProcessor for GLUE tasks"]},{"cell_type":"markdown","metadata":{"id":"mbyklwlbmfdY"},"source":["### Input Data\n","**To get concreteness and WAC vectors data from new sources do as follows:**\n","\n","- Write a parser function for your data.\n","- The parser function must return a dataframe with two columns `['word','measure']`\n","- Write an argument dictionary for your parser function. e.g. `arg={'fname':r\"c:\\raw_data.csv\"}`\n","- Call `tokenize.External_Data()` and provide its arguments. It returns a Data Series object of tokenized data\n","- You may use `wac` or `concreteness` function `.ds2dict()` to convert the data series to a dictionary, named `wac_dict`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUiduhEPmQEz"},"outputs":[],"source":["# Get Concreteness\n","concreteness_dict = concreteness.load_input(\"./inputs/uncased_inferred_conc.pkl\", tokenizer.name_or_path)\n","FULL_CONC = np.ceil(max(list(concreteness_dict.values())))\n","# Get WAC2VEC\n","wac_dict = wac.load_input(\"./inputs/clip_wac_513.pkl\", tokenizer.name_or_path)\n","# # Example of how to load a new WAC data\n","# from wac_bert_tools import parser\n","# fpath = r\"\"\n","# pars_clip = parser.clip_wac\n","# wac_ds = tokenize.External_Data(pars_clip, {'fname':fpath},'bert-base-uncased',False)\n","# wac_dict = wac.ds2dict(wac_ds, 513)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSrbnnTOu1rd"},"outputs":[],"source":["# Length of a WAC vector\n","WAC_LEN = next(len(v) for k,v in wac_dict.items())\n","assert WAC_LEN<= transformers.AutoModel.from_pretrained(model_checkpoint).config_class().hidden_size ,\\\n"," \"Size of WAC vectors must NOT be larger than BERT-like model. Use trim_func argument in wac.ds2dict\""]},{"cell_type":"markdown","metadata":{"id":"8ZAig4tcmfdZ"},"source":["### Training Data\n","\n","A `DataProcessor` class is reuqired for any new dataset. Any new `DataProcessor` must have the same functions with the same outputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDX_hCAvEn9h"},"outputs":[],"source":["class DataProcessor():\n","    \"\"\" Preprocesses a dataset and provides methods and properties required for running the notebook \"\"\"\n","    def __init__(self, task):\n","        GLUE_TASKS = ['cola','mnli', \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n","        assert task in GLUE_TASKS, 'The task is not a valid GLUE task'\n","        actual_task = 'mnli' if task =='mnli-mm' else task  # there are special cased for mnli\n","        self.task = task\n","        self.dataset = load_dataset('glue',actual_task)\n","        self.metric = load_metric('glue',actual_task)\n","\n","    def get_NUM_OUT(self):\n","        \"\"\" Returns number of classes in the labels\"\"\"\n","        return 3 if self.task.startswith('mnli') else 1 if self.task=='stsb' else 2\n","\n","    def get_metric_name(self):\n","        \"\"\"Returns the metric name for the task. Must be a key in get_metric_fn() output\"\"\"\n","        return  \"pearson\" if self.task == \"stsb\" else \"matthews_correlation\" if self.task == \"cola\" else \"accuracy\"\n","\n","    def get_metric_fn(self,guess,targs):\n","        \"\"\" guess: torch tensor of predictions (guesses)\n","          targs: torch tensor of labels (targets)\n","        Returns a dictionary of metric_name(s) and their values.\"\"\"\n","        if self.task != \"stsb\":\n","            guesses = torch.max(guess, dim=1)  # maximum value of each row of the input tensor\n","            targets = torch.max(targs, dim=1)  # like np argmax\n","            predictions = guesses.indices\n","            labels = targets.indices\n","        else:\n","            predictions = guess.squeeze()\n","            labels = targs.squeeze()\n","        return self.metric.compute(predictions=predictions, references=labels)\n","    \n","    def get_loss_fn(self):\n","        if self.get_NUM_OUT()==1:  \n","            _loss_fn = torch.nn.MSELoss()\n","        else:\n","            _loss_fn = torch.nn.BCEWithLogitsLoss()\n","        return _loss_fn\n","        \n","    def get_data(self):\n","        \"\"\"Returns a dictionary of 'train_X1', 'train_y', 'test_X1', 'test_y', 'test_X2' and 'train_X2'\n","              Values must be `numpy.ndarray` of compatible size. 'test_X2' and 'train_X2' values can be None.\n","              Arrays are examples of string sequences (sentences)\"\"\"\n","        dataz=defaultdict(lambda:None)\n","        task_to_keys = {\n","        \"cola\": (\"sentence\", None),\n","        \"mnli\": (\"premise\", \"hypothesis\"),\n","        \"mnli-mm\": (\"premise\", \"hypothesis\"),\n","        \"mrpc\": (\"sentence1\", \"sentence2\"),\n","        \"qnli\": (\"question\", \"sentence\"),\n","        \"qqp\": (\"question1\", \"question2\"),\n","        \"rte\": (\"sentence1\", \"sentence2\"),\n","        \"sst2\": (\"sentence\", None),\n","        \"stsb\": (\"sentence1\", \"sentence2\"),\n","        \"wnli\": (\"sentence1\", \"sentence2\"),\n","        }\n","        validation_key = \"validation_mismatched\" if self.task == \"mnli-mm\" else \"validation_matched\" if self.task == \"mnli\" else \"validation\"\n","        sentence1_key, sentence2_key = task_to_keys[self.task]\n","\n","        train_X1 = self.dataset['train'][sentence1_key]\n","        test_X1 = self.dataset[validation_key][sentence1_key]\n","        train_X1, test_X1 = [np.array(d) for d in [train_X1, test_X1]]\n","\n","        train_y = np.array(self.dataset['train']['label'])\n","        test_y = np.array(self.dataset[validation_key]['label'])\n","        if self.task!='stsb':\n","            enc = OneHotEncoder(sparse=False)\n","            train_y = enc.fit_transform(train_y.reshape(-1,1))\n","            test_y = enc.transform(test_y.reshape(-1,1))\n","        else:\n","            train_y = train_y/5\n","            test_y  = test_y/5\n","        dataz.update({'train_X1':train_X1, 'train_y':train_y, 'test_X1':test_X1,'test_y':test_y , })\n","        if sentence2_key:\n","            train_X2 = self.dataset['train'][sentence1_key]\n","            test_X2 = self.dataset[validation_key][sentence1_key]\n","            train_X2, test_X2 = [np.array(d) for d in [train_X2, test_X2]]\n","            dataz.update({'train_X2':train_X2,'test_X2':test_X2})\n","            \n","        return dataz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hegkBQLDEibz"},"outputs":[],"source":["myData = DataProcessor(task)\n","dataz = myData.get_data()\n","compute_metrics = myData.get_metric_fn\n","loss_fn = myData.get_loss_fn()\n","NUM_OUT = myData.get_NUM_OUT()\n","metric_name = myData.get_metric_name()\n","reg_arg['metric'] = metric_name\n","reg_arg['loss_name'] = str(loss_fn)"]},{"cell_type":"markdown","metadata":{"id":"bVdgPGvJmfdb"},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldke9Vswmfdb"},"outputs":[],"source":["class MultiLabelDataset(torch.utils.data.Dataset): \n","    \"\"\" The purpose of this class is to prepare our data for torch.utils.data.DataLoader\"\"\"\n","    def __init__(self, labels, tokenizer, max_len, text1, text2 = None):\n","        self.text1 = text1\n","        self.text2 = text2\n","        self.targets = torch.from_numpy(labels)\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len  # Max len of a sequence\n","        self.num_class = len(torch.unique(self.targets))\n","\n","    def __len__(self):  # Length of the dataset\n","        return len(self.targets)\n","\n","    def __getitem__(self, index):\n","        text1 = self.text1[index]\n","        text2 = self.text2[index] if self.text2 is not None else None\n","        # Encode plus adds “special tokens” which are special IDs the model uses and converts tokens into IDs which are understandable by the model\n","        inputs = self.tokenizer.encode_plus(\n","            text1,\n","            text2,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length', # len of inputs elements must be the same, 'cause of torch.stack method\n","            truncation=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']  # Token indicies, numerical representation of tokens that are building a sequence\n","        concreteness  = [concreteness_dict[i] for i in ids]\n","        wac_vector    = [torch.from_numpy(wac_dict[i]) for i in ids]\n","        mask = inputs['attention_mask']  # indicates to the model which tokens should be attended to and which should not\n","        # For models with classification and QA purposel, two sequences are to be encoded in the same input ID, hence they need to be seperated by token type\n","        token_type_ids = inputs[\"token_type_ids\"]  # \n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),  # Torch tensor is multi-dimensional matrix containing elements of a single data type\n","            'concreteness':torch.tensor(concreteness, dtype=torch.float),\n","            'wac_vector':torch.stack(wac_vector),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.long if self.num_class<=2 else torch.float)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QeXv92ihmfdc"},"outputs":[],"source":["training_data = MultiLabelDataset(dataz['train_y'],tokenizer, MAX_LEN, dataz['train_X1'], dataz['train_X2'])\n","test_data = MultiLabelDataset(dataz['test_y'],tokenizer, MAX_LEN, dataz['test_X1'], dataz['test_X2'])\n","\n","if TEST:\n","    lengths = [len(training_data)//10, len(training_data)-len(training_data)//10]\n","    training_data = torch.utils.data.random_split(training_data, lengths, generator=torch.Generator().manual_seed(2021))[0]\n","    lengths = [len(test_data)//10, len(test_data)-len(test_data)//10]\n","    test_data = torch.utils.data.random_split(test_data, lengths, generator=torch.Generator().manual_seed(2021))[0]\n","\n","train_params = {'batch_size': BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0,\n","                'worker_init_fn' : 0 if FIXED_SEED else None\n","                }\n","\n","test_params = {'batch_size': BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0,\n","                'worker_init_fn' : 0 if FIXED_SEED else None\n","                }    \n","\n","training_loader = torch.utils.data.DataLoader(training_data, **train_params)\n","testing_loader = torch.utils.data.DataLoader(test_data, **test_params)  # Returns batches of data"]},{"cell_type":"markdown","metadata":{"id":"wpaPiCHNmfdd"},"source":["## Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yPnsEq8XGO_"},"outputs":[],"source":["class BertLikeModel(torch.nn.Module): \n","    def __init__(self, model_checkpoint, NUM_OUT):\n","        super(BertLikeModel, self).__init__()\n","        self.bert = transformers.AutoModel.from_pretrained(model_checkpoint, num_labels = NUM_OUT,output_hidden_states=True)\n","        LAST_LYR_SIZE = self.bert.config_class().hidden_size\n","        if WAC_METHOD=='ADD' and MATCH_WAC:\n","            in_feature_size = WAC_LEN\n","        elif WAC_METHOD=='CAT':\n","            in_feature_size = LAST_LYR_SIZE+WAC_LEN+1  # 1 is concreteness\n","        else:\n","            in_feature_size = LAST_LYR_SIZE\n","\n","        self.dense_pool = torch.nn.Linear(in_feature_size, in_feature_size)\n","        self.activation_pool = torch.nn.ReLU()\n","        # Applies a linear transformation to the incoming data. Transforming the size of L1 output to a binary size for binary classification\n","        self.classifier = torch.nn.Linear(in_features=in_feature_size, out_features=NUM_OUT)  # Size of pooler is 768(Hidden)\n","        self.dropout = torch.nn.Dropout(DROPOUT)\n","        self.softmax = torch.nn.Softmax(dim=1)  # Force the output Tensor distribution to lie in the range [0,1] and sum to 1.\n","        self.resize = torch.nn.Linear(LAST_LYR_SIZE,WAC_LEN)\n","        self.prepooler = torch.nn.Linear(in_feature_size, in_feature_size)\n","\n","    def pooler(self,hidden_state):\n","        # We \"pool\" the model by simply taking the hidden state corresponding to the first token.\n","        first_token_tensor = hidden_state[:, 0]\n","        pooled_output = self.dense_pool(first_token_tensor)\n","        pooled_output = self.activation_pool(pooled_output)\n","        return pooled_output\n","    \n","    def apply_wac(self, hidden_state, wac, conc):\n","        if MATCH_WAC: wac = wac[:,:, :WAC_LEN]\n","        concrete_w = conc[:,:,None]\n","        abstract_w = FULL_CONC-concrete_w  # Assumes full weight for BERT when WAC is not valid\n","        if WAC_METHOD=='ADD':\n","            if MATCH_WAC: hidden_state = self.resize(hidden_state)\n","            hidden_state = (hidden_state*abstract_w + wac*concrete_w)/FULL_CONC \n","        elif WAC_METHOD=='CAT':\n","            hidden_state = torch.cat((hidden_state, wac, concrete_w ),2)\n","        else:\n","            if WAC_METHOD != '': raise ValueError('Not a valid WAC_METHOD')\n","        return self.prepooler(hidden_state)  # To bring the impact of WAC into picture to the first element\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, wac, conc):\n","        output_1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        #hidden state size: (batch_size, sequence_length, hidden_size)\n","        hidden_state = output_1[-1][-1] # Last hidden layer\n","        if WAC_METHOD: hidden_state = self.apply_wac(hidden_state, wac, conc)\n","        # Immitate BERT sequence classifier\n","        pooled = self.pooler(hidden_state)\n","        pooled = self.dropout(pooled)\n","        output = self.classifier(pooled)  # Transforms pooler to a e.g. binary \n","        return output.squeeze()"]},{"cell_type":"markdown","metadata":{"id":"SbijxyFRmfdg"},"source":["## Train and Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBhsZ4rwmfdg"},"outputs":[],"source":["def train(model, training_loader, optimizer):\n","    model.train()\n","    for data in tqdm(training_loader):\n","        ids = data['ids'].to(device, dtype = torch.long)  # ids.shape=batch_size*max_len\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float) \n","        wac = data['wac_vector'].to(device, dtype = torch.float)\n","        conc = data['concreteness'].to(device, dtype = torch.float)\n","        outputs = model(ids, mask, token_type_ids, wac, conc)  # Its calling forward here, output is distribution over 2 things  \n","        optimizer.zero_grad()\n","        loss = loss_fn(outputs.squeeze(), targets.squeeze())\n","        loss.backward()\n","        optimizer.step()\n","    return loss\n","    \n","def validation(model, testing_loader):\n","    model.eval()\n","    fin_targets=[]\n","    fin_outputs=[]\n","    with torch.no_grad():  # Context-manager that disabled gradient calculation.\n","        for data in tqdm(testing_loader):\n","            targets = data['targets']\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            wac = data['wac_vector'].to(device, dtype = torch.float)\n","            conc = data['concreteness'].to(device, dtype = torch.float)\n","            outputs = model(ids, mask, token_type_ids, wac, conc)\n","            if NUM_OUT!=1: outputs = torch.sigmoid(outputs)\n","            outputs = outputs.cpu().detach()  # Converts to 0 and 1 results instead of doing a loss and backward step, for multiple classes something like argmax is needed!\n","            fin_outputs.extend(outputs)\n","            fin_targets.extend(targets)\n","    return torch.stack(fin_outputs), torch.stack(fin_targets)  # Concatenates a sequence of tensors along a new dimension\n","def set_value(val, col):\n","    reg = pd.read_csv('model_registry.csv')\n","    reg[-1:][col] = val\n","    reg.to_csv('model_registry.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PqI8E4Y1mfdh","scrolled":false},"outputs":[],"source":["#%debug\n","torch.cuda.empty_cache()\n","model = BertLikeModel(model_checkpoint,NUM_OUT)\n","model.to(device)    \n","optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)\n","print(\"Models with no study name won't be saved.\")\n","reg = registrar.register(**reg_arg)\n","metric_lt = []\n","for epoch in range(EPOCHS):\n","    loss = train(model, training_loader, optimizer)\n","    print(f'Epoch: {epoch}, Loss:  {loss.item()}')  \n","    guess, targs = validation(model, testing_loader)\n","    metric_value = compute_metrics(guess, targs)\n","    metric_lt.append(metric_value[metric_name])\n","    print('{} of test set'.format(metric_value))\n","max_metric = max(metric_lt)\n","registrar.update_metric(max_metric,reg['fDir'])\n","max_epoch = metric_lt.index(max_metric)+1\n","set_value(max_epoch, 'epoch')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sO1BkE6Imfdi"},"outputs":[],"source":["if reg['study_name']: pkl.dump(model, open('./models/%s.sav'%reg['study_name'],'wb'))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"WAC_BERT_Pipeline.ipynb","provenance":[]},"interpreter":{"hash":"67506916ef47f0be198714fa4083b4b393af43353b733d0c22133da6f992dfd0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"metadata":{"interpreter":{"hash":"8c56b8c11e915e338c3d70e26252c1d074842c945a99b07158016ecddcf83353"}}},"nbformat":4,"nbformat_minor":0}